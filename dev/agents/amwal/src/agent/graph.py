from typing import List, TypedDict, Callable, Optional
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage, BaseMessage
from langchain_openai import ChatOpenAI
from langgraph.graph import StateGraph, MessagesState
import json
from dotenv import load_dotenv
from langchain_core.callbacks.base import AsyncCallbackHandler

# ØªØ£ÙƒØ¯ Ù…Ù† ØªØ­Ù…ÙŠÙ„ Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø¨ÙŠØ¦Ø© Ù„Ù€ API Key Ø§Ù„Ø®Ø§Øµ Ø¨Ùƒ
load_dotenv()

class FinancialAssistantState(TypedDict):
    messages: List[BaseMessage]
    parsed_message: str
    mock_data: Optional[dict]
    stream_callback: Optional[Callable[[str], None]]

# ğŸ”¸ ØªÙ‡ÙŠØ¦Ø© LLM (Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù„ØºÙˆÙŠ)
llm = ChatOpenAI(
    model="gpt-4o",
    streaming=True
)

class StreamCaptureHandler(AsyncCallbackHandler):
    def __init__(self, on_token):
        self.on_token = on_token
        print(f"StreamCaptureHandler: Initialized with on_token callback: {on_token}")

    async def on_llm_new_token(self, token: str, **kwargs) -> None:
        """Called when the LLM generates a new token"""
        print(f"StreamCaptureHandler: on_llm_new_token called. Token received: '{token[:10]}...'") # Print a snippet
        if self.on_token:
            await self.on_token(token)
        else:
            print("StreamCaptureHandler: Warning - on_token callback is None.")

# ğŸ”¸ Ø§Ù„Ø¹Ù‚Ø¯Ø© 1: Ù…Ø­Ù„Ù„ Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª (Input Parser)
def parse_node(state: FinancialAssistantState) -> FinancialAssistantState:
    print("\n--- Entering parse_node ---")
    last_user = next((m for m in reversed(state["messages"]) if m.type == "human"), None)
    message_text = last_user.content if last_user else ""

    # fallback
    message = message_text
    mock_data = state.get("mock_data")  # default from state

    try:
        parsed = json.loads(message_text)
        message = parsed.get("message", message_text)
        # Only override if mock_data was provided in the message JSON
        if parsed.get("mock_data") is not None:
            mock_data = parsed["mock_data"]
        print("Parser: Input was valid JSON. Extracted message and mock_data.")
    except json.JSONDecodeError:
        print("Parser: Input is not JSON, keeping upstream mock_data.")
    except Exception as e:
        print(f"Parser: Unexpected error: {e}, keeping upstream mock_data.")

    return {
        "messages": state["messages"],
        "parsed_message": message,
        "mock_data": mock_data,
        "stream_callback": state.get("stream_callback")
    }

async def abdurrahman_node(state: FinancialAssistantState) -> FinancialAssistantState:
    print("\n--- Entering abdurrahman_node ---")

    default_data = {
        "name": "Ø¹Ø¨Ø¯Ø§Ù„Ù„Ù‡",
        "monthly_income": 9000,
        "financial_goal": "Ø´Ø±Ø§Ø¡ Ø³ÙŠØ§Ø±Ø© Ø¬Ø¯ÙŠØ¯Ø© Ø®Ù„Ø§Ù„ 6 Ø£Ø´Ù‡Ø±",
        "current_savings": 5000,
        "monthly_commitments": 3200,
        "financial_discipline_score": "86%",
        "credit_score": "23%",
        "transactions": [
            {"date": "2025-07-01", "description": "Ø±Ø§ØªØ¨ Ø´Ù‡Ø±ÙŠ", "amount": +9000},
            {"date": "2025-07-02", "description": "Ø¥ÙŠØ¬Ø§Ø± Ø´Ù‚Ø©", "amount": -2500},
            {"date": "2025-07-03", "description": "ÙÙˆØ§ØªÙŠØ± ÙƒÙ‡Ø±Ø¨Ø§Ø¡ ÙˆÙ…Ø§Ø¡", "amount": -400},
            {"date": "2025-07-04", "description": "Ù‚Ù‡ÙˆØ© ÙŠÙˆÙ…ÙŠØ©", "amount": -45},
            {"date": "2025-07-06", "description": "Ù…Ø·Ø¹Ù…", "amount": -120},
            {"date": "2025-07-08", "description": "Ø§Ø¯Ø®Ø§Ø± ØªÙ„Ù‚Ø§Ø¦ÙŠ", "amount": -800}
        ]
    }

    mock_data = state.get("mock_data") or default_data
    print(f"Abdurrahman: Using mock data: {mock_data is not None}")

    sys_msg = SystemMessage(content=(
        """Ø£Ù†Øª Ø¹Ø¨Ø¯Ø§Ù„Ø±Ø­Ù…Ù†ØŒ Ø§Ù„Ù…Ø³ØªØ´Ø§Ø± Ø§Ù„Ù…Ø§Ù„ÙŠ Ø§Ù„Ø°ÙƒÙŠ Ø¯Ø§Ø®Ù„ ØªØ·Ø¨ÙŠÙ‚ Ø£Ù…ÙˆØ§Ù„.
ØªÙ… ØªØ·ÙˆÙŠØ± Ø¹Ø¨Ø¯Ø§Ù„Ø±Ø­Ù…Ù† ÙˆØªØ·Ø¨ÙŠÙ‚ Ø£Ù…ÙˆØ§Ù„ Ø¨ÙˆØ§Ø³Ø·Ø© **Ø¹Ø¨Ø¯Ø§Ù„Ø±Ø­Ù…Ù† Ø­Ø³ÙŠÙ†**: Ù…Ø¤Ø³Ø³ GeniusAICo ÙˆÙ…Ù‡Ù†Ø¯Ø³ ØªØ¹Ù„Ù… Ø¢Ù„Ø© | Ù…Ù‡Ù†Ø¯Ø³ Ø£Ù†Ø¸Ù…Ø© Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠØŒ Ø¨Ù‡Ø¯Ù Ø¨Ù†Ø§Ø¡ Ø­Ù„ÙˆÙ„ Ø°ÙƒÙŠØ© Ù…Ø¤ØªÙ…ØªØ© Ù„Ù…Ø³ØªÙ‚Ø¨Ù„ Ø§Ù„Ø£Ø¹Ù…Ø§Ù„. Ù‡Ø°Ø§ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹ Ù‡Ùˆ Ù…Ø´Ø§Ø±ÙƒØ© ÙÙŠ **Ù‡Ø§ÙƒØ§Ø«ÙˆÙ† Ø£Ù…Ø¯ ÙÙ†ØªÙƒ** Ø§Ù„Ø°ÙŠ ÙŠÙ‚Ø§Ù… Ø¨Ø§Ù„Ø´Ø±Ø§ÙƒØ© Ø¨ÙŠÙ† Ø§Ù„Ø¥Ù†Ù…Ø§Ø¡ ÙˆØ£ÙƒØ§Ø¯ÙŠÙ…ÙŠØ© Ø·ÙˆÙŠÙ‚ØŒ Ø¶Ù…Ù† Ù…Ø³Ø§Ø± **Ø§Ù„ØªØ¹Ù„ÙŠÙ… Ø§Ù„Ù…Ø§Ù„ÙŠ (Ø§Ù„ØªÙˆØ¹ÙŠØ©)**ØŒ Ø¨Ù‡Ø¯Ù Ø±ÙØ¹ Ø§Ù„ÙˆØ¹ÙŠ Ø§Ù„Ù…Ø§Ù„ÙŠ Ù„Ø¯Ù‰ Ø§Ù„Ø£ÙØ±Ø§Ø¯ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ø¯ÙˆØ§Øª Ù…Ø¨ØªÙƒØ±Ø© Ù„Ù…ÙØ§Ù‡ÙŠÙ… Ø§Ù„Ø§Ø¯Ø®Ø§Ø± ÙˆØ§Ù„Ù…ÙŠØ²Ø§Ù†ÙŠØ© ÙˆØ§Ù„Ø§Ø¦ØªÙ…Ø§Ù†.

Ø£Ù†Øª ØªØªØ­Ø¯Ø« Ù…Ø¹ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø¨Ø£Ø³Ù„ÙˆØ¨ ÙˆØ¯ÙˆØ¯ ÙˆØ´Ø®ØµÙŠ **ÙƒØ£Ù†Ùƒ Ø£Ø®ÙˆÙ‡ Ø§Ù„ÙƒØ¨ÙŠØ± Ø§Ù„Ù„ÙŠ ÙŠÙ‡Ù…Ù‡ Ù…ØµÙ„Ø­ØªÙ‡ ÙˆÙŠÙÙ‡Ù… Ø¹Ù„ÙŠÙ‡ Ø²ÙŠÙ†**ØŒ Ù‡Ø¯ÙÙƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙ‡ ÙŠÙÙ‡Ù… ÙˆØ¶Ø¹Ù‡ Ø§Ù„Ù…Ø§Ù„ÙŠØŒ ÙˆÙŠØ­Ù‚Ù‚ Ø£Ù‡Ø¯Ø§ÙÙ‡ Ø¨Ø®Ø·Ø· Ø¹Ù…Ù„ÙŠØ© Ø¨Ø³ÙŠØ·Ø© **ÙˆØ¨ÙƒÙ„Ø§Ù… ÙŠØ¯Ø®Ù„ Ø§Ù„Ù‚Ù„Ø¨**.

ØªØ¹Ù„ÙŠÙ…Ø§Øª Ù‡Ø§Ù…Ø© Ù„Ø£Ø³Ù„ÙˆØ¨Ùƒ:
1. Ø§Ø¬Ø¹Ù„ Ø¥Ø¬Ø§Ø¨Ø§ØªÙƒ Ù‚ØµÙŠØ±Ø© ÙˆÙˆØ§Ø¶Ø­Ø©ØŒ ÙˆØªÙƒÙ„Ù… Ø¨Ø§Ù„Ù„Ù‡Ø¬Ø© Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠØ© **Ù…Ø«Ù„ Ù…Ø§ ÙŠØªÙƒÙ„Ù…ÙˆÙ† Ø§Ù„Ø´Ø¨Ø§Ø¨**.
2. Ø§Ø³ØªØ®Ø¯Ù… Ù„Ù‡Ø¬Ø© ØªØ´Ø¬ÙŠØ¹ÙŠØ© ÙˆÙ…Ø¨Ø§Ø´Ø±Ø© **Ù…Ù„ÙŠØ§Ù†Ø© Ø­Ù…Ø§Ø³ ÙˆØ¥ÙŠØ¬Ø§Ø¨ÙŠØ©**.
3. Ø§Ù‚ØªØ±Ø­ Ù†ØµØ§Ø¦Ø­ Ø¹Ù…Ù„ÙŠØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§ØªÙ‡ Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠØ© ÙÙ‚Ø· **Ø´ÙŠØ¡ ÙˆØ§Ù‚Ø¹ÙŠ ÙˆÙ…ÙÙŠØ¯ Ù„Ù‡**.
4. Ø¥Ø°Ø§ Ø³Ø£Ù„ Ø¹Ù† Ø´ÙŠØ¡ ØºÙŠØ± Ù…Ø§Ù„ÙŠØŒ Ø°ÙƒÙ‘Ø±Ù‡ Ø¨Ù„Ø·Ø§ÙØ© Ø£Ù† ØªØ±ÙƒÙŠØ²Ù†Ø§ Ù…Ø§Ù„ÙŠ Ø¨Ø­Øª **ÙˆØ£Ù† Ù‡Ù†Ø§ ØªØ®ØµØµÙ†Ø§ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ**.

ğŸ“Š Ù‡Ø°Ù‡ Ø¨ÙŠØ§Ù†Ø§ØªÙ‡ Ø§Ù„Ø­Ø§Ù„ÙŠØ©:
"""
            f"{json.dumps(mock_data, ensure_ascii=False, indent=2)}\n\n"
            "Ø§Ø¹ØªÙ…Ø¯ Ø¹Ù„ÙŠÙ‡Ø§ Ù„Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø£ÙŠ Ø³Ø¤Ø§Ù„ ÙŠØ·Ø±Ø­Ù‡ Ø§Ù„Ø¢Ù†."
    ))
    print(f"Abdurrahman: System message created.")

    messages_for_llm = [sys_msg] + state["messages"]
    print(f"Abdurrahman: Total messages for LLM: {len(messages_for_llm)}")
    if any(m.type == "human" for m in messages_for_llm):
        print(f"Abdurrahman: Last user message content: '{[m for m in messages_for_llm if m.type == 'human'][-1].content}'")

    print("Abdurrahman: Calling llm.ainvoke. Expecting auto-streaming if graph is streaming.")
    
    # âœ… Ø§Ù„ØªØºÙŠÙŠØ± Ø§Ù„Ø­Ø§Ø³Ù…: Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¯Ø§Ù„Ø© Ø§Ù„Ù€ callback Ù…Ù† Ø§Ù„Ù€ state
    stream_callback_func = state.get("stream_callback")
    callbacks = []
    if stream_callback_func:
        # âœ… Ø¥Ù†Ø´Ø§Ø¡ Ù†Ø³Ø®Ø© Ù…Ù† StreamCaptureHandler Ù…Ø¹ Ø¯Ø§Ù„Ø© Ø§Ù„Ù€ callback
        callbacks.append(StreamCaptureHandler(stream_callback_func))
        print("Abdurrahman: Attached StreamCaptureHandler to LLM call.")

    # âœ… ØªÙ…Ø±ÙŠØ± Ø§Ù„Ù€ callbacks Ø¥Ù„Ù‰ llm.ainvoke Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ÙˆØ³ÙŠØ· 'config'
    response = await llm.ainvoke(messages_for_llm, config={"callbacks": callbacks})
    
    print(f"Abdurrahman: LLM ainvoke completed. Response content length: {len(response.content)}")
    
    # Return the updated state, appending the full AI message
    # and ensuring all other custom state fields are propagated.

    return {
        "messages": state["messages"] + [response],
        "parsed_message": state["parsed_message"],
        "mock_data": state["mock_data"],
        "stream_callback": state["stream_callback"] # KEEP propagating this, as main.py relies on it being in the state
    }


# ğŸ”¸ Ø§Ù„Ø¹Ù‚Ø¯Ø© 3: Ø§Ù„Ù…Ù†Ù‡ÙŠ (Finalizer) - No change needed here
def final_node(state: FinancialAssistantState) -> FinancialAssistantState:
    print("\n--- Entering final_node ---")
    print(f"Finalizer: Final messages count: {len(state['messages'])}")
    print(f"Finalizer: Last message type: {state['messages'][-1].type if state['messages'] else 'None'}")
    print(f"Finalizer: Stream callback still present: {state.get('stream_callback') is not None}")
    return state

# ... (rest of your graph.py file, including builder and graph compilation)

## **Ø¨Ù†Ø§Ø¡ ÙˆØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ø¬Ø±Ø§Ù (Ø§Ù„Ø±Ø³Ù… Ø§Ù„Ø¨ÙŠØ§Ù†ÙŠ)**
builder = StateGraph(FinancialAssistantState)

builder.add_node("parser", parse_node)
builder.add_node("abdurrahman", abdurrahman_node)
builder.add_node("final", final_node)

builder.set_entry_point("parser")

builder.add_edge("parser", "abdurrahman")
builder.add_edge("abdurrahman", "final")

graph = builder.compile()